{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPaVGqROUWyAlQZWYvS9wFc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/03axdov/Notebooks/blob/main/SentimentClassficiationII.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "82EVTQWC_jDh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "tfds.disable_progress_bar()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, metric):\n",
        "  plt.plot(history.history[metric], label=metric)\n",
        "  plt.plot(history.history[\"val_\" + metric], label=\"val_\"+metric)\n",
        "  plt.ylabel(metric)\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.legend(loc=\"upper left\")"
      ],
      "metadata": {
        "id": "28CIltHA_tAU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset, info = tfds.load(\n",
        "    \"imdb_reviews\", with_info=True,\n",
        "    as_supervised=True,\n",
        ")\n",
        "\n",
        "train_dataset, test_dataset = dataset[\"train\"], dataset[\"test\"]"
      ],
      "metadata": {
        "id": "GfsN8LcJAHOt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59444bf9-8762-4ee8-a153-a5e49e2c7e39"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDownloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n",
            "\u001b[1mDataset imdb_reviews downloaded and prepared to ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "dataset = iter(train_dataset)\n",
        "while i < 3:\n",
        "  text, label = next(dataset)\n",
        "  print(str(text.numpy().decode()))\n",
        "  print(label.numpy())\n",
        "  i += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zz9E73FnB7CE",
        "outputId": "ebc58c0a-f58f-4834-b8ed-f580bbd47e68"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\n",
            "0\n",
            "I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.\n",
            "0\n",
            "Mann photographs the Alberta Rocky Mountains in a superb fashion, and Jimmy Stewart and Walter Brennan give enjoyable performances as they always seem to do. <br /><br />But come on Hollywood - a Mountie telling the people of Dawson City, Yukon to elect themselves a marshal (yes a marshal!) and to enforce the law themselves, then gunfighters battling it out on the streets for control of the town? <br /><br />Nothing even remotely resembling that happened on the Canadian side of the border during the Klondike gold rush. Mr. Mann and company appear to have mistaken Dawson City for Deadwood, the Canadian North for the American Wild West.<br /><br />Canadian viewers be prepared for a Reefer Madness type of enjoyable howl with this ludicrous plot, or, to shake your head in disgust.\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "SF2Sya6uEMf9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "cuVN8Vn2EQms"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for example, label in train_dataset.take(1):\n",
        "  print('texts: ', example.numpy()[:3])\n",
        "  print()\n",
        "  print('labels: ', label.numpy()[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0oVsRC2EoIl",
        "outputId": "6afa7b3a-bc9d-49c8-ec47-75f49b5937d1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "texts:  [b'I\\'m hearing rumors of an upcoming \"Leonard Nimoy Demonstrates the Blu-ray Disc\". With advances over the past 25 years ranging from Steady-cam to CGI, it\\'ll be interesting to see if the franchise can be reinvigorated. I just hope it helps to remove the bad taste left in my mouth by that whole Magnavision demonstration fiasco.<br /><br />And yes... \"Leonard Nimoy Demonstrates the Betamax VCR\" was a brilliant milestone in entertainment history. After the tentative \"Leonard Nimoy Demonstrates the Compact Cassette\" and the downright tacky \"Leonard Nimoy Demonstrates the 8-Track Tape\", who would have expected such a glorious piece of cinema? I\\'m weeping right now just thinking about it.'\n",
            " b'I remember my parents not understanding Saturday Night Live when I was 15. They also did not understand Rock n Roll and many other things. Now that I am approaching their age, I still remember, and find I understand many of the things my kids love. But this is pathetic. I cannot say I have seen any by Sarah except for a few appearances here and there. They were reasonable. I do not see her as anything special. But this show is just so far below what I expected from her. The IMDb write up made it sound like potential. So, just for that, I started watching the first episode. I turned it off half way through. Anything else is better that that. Jokes that are meant for a 5 year old presented on a supposed adult program. Well, Sarah, this adult is inly moved to turn you off. I just cant believe that someone actually financed this insult to comedy. Only good thing I can say is that there are sooooo many bad jokes deposited here, saving other shows from such an embarrassment.'\n",
            " b'This might be the poorest example of amateur propaganda ever made. The writers and producers should study the German films of the thirties and forties. They knew how to sell. Even soviet-style clunky leader as god-like father-figure were better done. Disappointing. The loss of faith, regained in church at last second just in time for daddy to be \"saved\" by the Hoover/God was not too bad. Unfortunately, it seemed rushed and not nearly melodramatic enough. A few misty heavenlier shots of the angelical Hoover up in the corner of the screen-beaming and nodding- would have added a lot. The best aspect is Hoover only saving the deserving family and children WHO had \"proven\" their worth. Unfortunately, other poor homeless were portrayed as likable and even good- yet the Hoover-God doesn\\'t help them. A better approach would have been shots of them drinking spirits to show the justice of their condition. Finally, bright and cheerful scenes of recovery (after Hoover saved the country from the depression) should have rolled at the end. We could see then how Hoover-God had saved not just THIS deserving family, but all the truly deserving. Amateurist at best.']\n",
            "\n",
            "labels:  [0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 1000\n",
        "encoder = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE\n",
        ")\n",
        "encoder.adapt(train_dataset.map(lambda x, y: x))"
      ],
      "metadata": {
        "id": "Rx0R5gTuF3fM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = np.array(encoder.get_vocabulary())\n",
        "vocab[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sxb0A2j_GJ-E",
        "outputId": "8219e756-c758-4d50-c0b3-5e0b294dfbaf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i',\n",
              "       'this', 'that', 'br', 'was', 'as', 'for', 'with', 'movie', 'but'],\n",
              "      dtype='<U14')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_example = encoder(example)[:3].numpy()\n",
        "encoded_example"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cop6us5VGXjr",
        "outputId": "11ba1b86-4f14-4aca-ad62-fdf6107b261b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[142,   1,   1, ...,   0,   0,   0],\n",
              "       [ 10, 364,  56, ...,   0,   0,   0],\n",
              "       [ 11, 227,  28, ...,   0,   0,   0]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for n in range(3):\n",
        "  print(f\"Original: {example[n].numpy()}\")\n",
        "  print(f\"Round trip: {' '.join(vocab[encoded_example[n]])}\")\n",
        "  print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSbcm7zoHpnl",
        "outputId": "148d8899-5903-44e4-fe7c-b91733a08252"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: b'I\\'m hearing rumors of an upcoming \"Leonard Nimoy Demonstrates the Blu-ray Disc\". With advances over the past 25 years ranging from Steady-cam to CGI, it\\'ll be interesting to see if the franchise can be reinvigorated. I just hope it helps to remove the bad taste left in my mouth by that whole Magnavision demonstration fiasco.<br /><br />And yes... \"Leonard Nimoy Demonstrates the Betamax VCR\" was a brilliant milestone in entertainment history. After the tentative \"Leonard Nimoy Demonstrates the Compact Cassette\" and the downright tacky \"Leonard Nimoy Demonstrates the 8-Track Tape\", who would have expected such a glorious piece of cinema? I\\'m weeping right now just thinking about it.'\n",
            "Round trip: im [UNK] [UNK] of an [UNK] [UNK] [UNK] [UNK] the [UNK] [UNK] with [UNK] over the past [UNK] years [UNK] from [UNK] to [UNK] [UNK] be interesting to see if the [UNK] can be [UNK] i just hope it [UNK] to [UNK] the bad [UNK] left in my [UNK] by that whole [UNK] [UNK] [UNK] br and yes [UNK] [UNK] [UNK] the [UNK] [UNK] was a brilliant [UNK] in entertainment history after the [UNK] [UNK] [UNK] [UNK] the [UNK] [UNK] and the [UNK] [UNK] [UNK] [UNK] [UNK] the [UNK] [UNK] who would have expected such a [UNK] piece of cinema im [UNK] right now just thinking about it                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
            "\n",
            "Original: b'I remember my parents not understanding Saturday Night Live when I was 15. They also did not understand Rock n Roll and many other things. Now that I am approaching their age, I still remember, and find I understand many of the things my kids love. But this is pathetic. I cannot say I have seen any by Sarah except for a few appearances here and there. They were reasonable. I do not see her as anything special. But this show is just so far below what I expected from her. The IMDb write up made it sound like potential. So, just for that, I started watching the first episode. I turned it off half way through. Anything else is better that that. Jokes that are meant for a 5 year old presented on a supposed adult program. Well, Sarah, this adult is inly moved to turn you off. I just cant believe that someone actually financed this insult to comedy. Only good thing I can say is that there are sooooo many bad jokes deposited here, saving other shows from such an embarrassment.'\n",
            "Round trip: i remember my parents not [UNK] [UNK] night live when i was [UNK] they also did not understand rock [UNK] [UNK] and many other things now that i am [UNK] their age i still remember and find i understand many of the things my kids love but this is [UNK] i cannot say i have seen any by [UNK] except for a few [UNK] here and there they were [UNK] i do not see her as anything special but this show is just so far [UNK] what i expected from her the imdb write up made it sound like potential so just for that i started watching the first episode i turned it off half way through anything else is better that that jokes that are meant for a 5 year old [UNK] on a supposed [UNK] [UNK] well [UNK] this [UNK] is [UNK] [UNK] to turn you off i just cant believe that someone actually [UNK] this [UNK] to comedy only good thing i can say is that there are [UNK] many bad jokes [UNK] here [UNK] other shows from such an [UNK]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
            "\n",
            "Original: b'This might be the poorest example of amateur propaganda ever made. The writers and producers should study the German films of the thirties and forties. They knew how to sell. Even soviet-style clunky leader as god-like father-figure were better done. Disappointing. The loss of faith, regained in church at last second just in time for daddy to be \"saved\" by the Hoover/God was not too bad. Unfortunately, it seemed rushed and not nearly melodramatic enough. A few misty heavenlier shots of the angelical Hoover up in the corner of the screen-beaming and nodding- would have added a lot. The best aspect is Hoover only saving the deserving family and children WHO had \"proven\" their worth. Unfortunately, other poor homeless were portrayed as likable and even good- yet the Hoover-God doesn\\'t help them. A better approach would have been shots of them drinking spirits to show the justice of their condition. Finally, bright and cheerful scenes of recovery (after Hoover saved the country from the depression) should have rolled at the end. We could see then how Hoover-God had saved not just THIS deserving family, but all the truly deserving. Amateurist at best.'\n",
            "Round trip: this might be the [UNK] example of [UNK] [UNK] ever made the writers and [UNK] should [UNK] the [UNK] films of the [UNK] and [UNK] they knew how to [UNK] even [UNK] [UNK] [UNK] as [UNK] [UNK] were better done [UNK] the [UNK] of [UNK] [UNK] in [UNK] at last second just in time for [UNK] to be [UNK] by the [UNK] was not too bad unfortunately it seemed [UNK] and not nearly [UNK] enough a few [UNK] [UNK] shots of the [UNK] [UNK] up in the [UNK] of the [UNK] and [UNK] would have [UNK] a lot the best [UNK] is [UNK] only [UNK] the [UNK] family and children who had [UNK] their worth unfortunately other poor [UNK] were portrayed as [UNK] and even good yet the [UNK] doesnt help them a better [UNK] would have been shots of them [UNK] [UNK] to show the [UNK] of their [UNK] finally [UNK] and [UNK] scenes of [UNK] after [UNK] [UNK] the country from the [UNK] should have [UNK] at the end we could see then how [UNK] had [UNK] not just this [UNK] family but all the truly [UNK] [UNK] at best                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=len(encoder.get_vocabulary()),\n",
        "        output_dim=64,\n",
        "        mask_zero=True\n",
        "    ),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])"
      ],
      "metadata": {
        "id": "gRRwQLdodUx8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3MBeLGvgEBc",
        "outputId": "73ee5320-e410-48df-8916-0aba016a1b49"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization (TextVec  (None, None)             0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 64)          64000     \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 128)              66048     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 138,369\n",
            "Trainable params: 138,369\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print([layer.supports_masking for layer in model.layers])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03h9E6bhe9Yk",
        "outputId": "bfd8b9ee-560f-48a0-8bcc-7bc78f73d869"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[False, True, True, True, True]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = (\"The movie was cool. Great acting and interesting plot lines. Would definitely recommend this movie!\")\n",
        "\n",
        "predictions = model.predict(np.array([sample_text]))\n",
        "print(predictions[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NozleLzCfJ2U",
        "outputId": "f2cb1f5f-cbf9-471d-9cf0-057cb3f8c146"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.01335325]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padding = \"the\" * 2000\n",
        "predictions = model.predict(np.array([sample_text, padding]))\n",
        "print(predictions[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJRJjvyXfeMX",
        "outputId": "f43a9583-b874-432a-825f-bcf2cd12df23"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.01335325]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "metadata": {
        "id": "Q2BbLmqsesbd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=test_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    validation_steps=30\n",
        ")"
      ],
      "metadata": {
        "id": "oIDGgkJre1GM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}